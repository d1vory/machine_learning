{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'soundfile'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "\n",
    "import pandas as pd\n",
    "import torchmetrics as torchmetrics\n",
    "\n",
    "str(torchaudio.get_audio_backend())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset \n",
    "As part of HW2, you will work with the audio dataset. Download the dataset by the following [link](https://urbansounddataset.weebly.com/download-urbansound8k.html) and remember a path to it. You will have to train a sound classification model, explore different audio transformation methods and try different model architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UrbanSoundDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,\n",
    "                 annotations_file,\n",
    "                 audio_dir,\n",
    "                 transformation,\n",
    "                 target_sample_rate,\n",
    "                 num_samples,\n",
    "                 device):\n",
    "        self.annotations = pd.read_csv(annotations_file)\n",
    "        self.audio_dir = audio_dir\n",
    "        self.device = device\n",
    "        self.transformation = transformation.to(self.device)\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        audio_sample_path = self._get_audio_sample_path(index)\n",
    "        label = self._get_audio_sample_label(index)\n",
    "        signal, sr = torchaudio.load(audio_sample_path)\n",
    "        signal = signal.to(self.device)\n",
    "        signal = self._resample_if_necessary(signal, sr)\n",
    "        signal = self._mix_down_if_necessary(signal)\n",
    "        signal = self._cut_if_necessary(signal)\n",
    "        signal = self._right_pad_if_necessary(signal)\n",
    "        signal = self.transformation(signal)\n",
    "        return signal, label\n",
    "\n",
    "    def _cut_if_necessary(self, signal):\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            signal = signal[:, :self.num_samples]\n",
    "        return signal\n",
    "\n",
    "    def _right_pad_if_necessary(self, signal):\n",
    "        length_signal = signal.shape[1]\n",
    "        if length_signal < self.num_samples:\n",
    "            num_missing_samples = self.num_samples - length_signal\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
    "        return signal\n",
    "\n",
    "    def _resample_if_necessary(self, signal, sr):\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
    "            signal = resampler(signal)\n",
    "        return signal\n",
    "\n",
    "    def _mix_down_if_necessary(self, signal):\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        return signal\n",
    "\n",
    "    def _get_audio_sample_path(self, index):\n",
    "        fold = f\"fold{self.annotations.iloc[index, 5]}\"\n",
    "        path = os.path.join(self.audio_dir, fold, self.annotations.iloc[\n",
    "            index, 0])\n",
    "        return path\n",
    "\n",
    "    def _get_audio_sample_label(self, index):\n",
    "        return self.annotations.iloc[index, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the dataset \n",
    "ANNOTATIONS_FILE = \"datasets/UrbanSound8K/metadata/UrbanSound8K.csv\"\n",
    "AUDIO_DIR = \"datasets/UrbanSound8K/audio\"\n",
    "\n",
    "# audio signal sample rate \n",
    "SAMPLE_RATE = 22050\n",
    "# max number of samples in audio\n",
    "NUM_SAMPLES = 22050\n",
    "\n",
    "# optimizer learning rate\n",
    "LEARNING_RATE = 1e-5\n",
    "# number of train epochs\n",
    "EPOCHS = 10\n",
    "# number of samples in each batch size\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining audio preprocessing function. Refer to torchaudio.transforms to \n",
    "# check Spectrogram(), MFCC() and LFCC() transfrormations. Retrain the model\n",
    "# with each transformation and write your metrics resulst and conclusaions.\n",
    "mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=SAMPLE_RATE,\n",
    "        n_fft=1024,\n",
    "        hop_length=512,\n",
    "        n_mels=64\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset len:  8732\n",
      "Train split: 6985 | Validation split: 873 | Test split: 874\n"
     ]
    }
   ],
   "source": [
    "# defining dataset class with \n",
    "dataset = UrbanSoundDataset(\n",
    "    ANNOTATIONS_FILE,\n",
    "    AUDIO_DIR,\n",
    "    mel_spectrogram,\n",
    "    SAMPLE_RATE,\n",
    "    NUM_SAMPLES,\n",
    "    device\n",
    ")\n",
    "\n",
    "# calculating validation split sizes\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "val_test_size = len(dataset) - train_size\n",
    "\n",
    "val_size = val_test_size // 2\n",
    "test_size = val_test_size - val_size\n",
    "\n",
    "print('Dataset len: ', len(dataset))\n",
    "print(f'Train split: {train_size} | Validation split: {val_size} | Test split: {test_size}')\n",
    "\n",
    "# splitting original dataset into train and val_test sets\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, val_size + test_size]\n",
    ")\n",
    "# splitting val_test set into val dataset and test dataset\n",
    "val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    val_dataset, [val_size, test_size]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal, label = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Define your model architecture here. \n",
    "\n",
    "1) Build a baseline model that consists of consecutive convolution blocks. Flatten the convolution output and pass through the linear layer with softmax activation function, to obtain class distributions\n",
    "\n",
    "2) Add the BatchNorm layer after each convolution block and compare the results with the baseline model\n",
    "\n",
    "3) Add a Dropout layer after each BatchNorm block and compare the results\n",
    "\n",
    "4) Try different parameters for the blocks:\n",
    " - Conv layer: out_channels, kernel_size, stride\n",
    " - Dropout: p\n",
    " \n",
    " and compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class CLModel to define your model architecture and computational graph\n",
    "class CLModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ################\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        # define you convolution based classification model\n",
    "        # 4 conv blocks / flatten / linear / softmax\n",
    "\n",
    "        ################\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels=1,\n",
    "                    out_channels=16,\n",
    "                    kernel_size=(3,3),\n",
    "                    padding=2\n",
    "                ),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2),\n",
    "                nn.BatchNorm2d(16)\n",
    "            )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=16,\n",
    "                out_channels=32,\n",
    "                kernel_size=(3,3),\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.BatchNorm2d(32)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64,\n",
    "                kernel_size=(3,3),\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.BatchNorm2d(64)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=128,\n",
    "                kernel_size=(3,3),\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.BatchNorm2d(128)\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(2560, 10)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "\n",
    "    def forward(self, input_data: torch.Tensor) -> torch.Tensor:\n",
    "        ################\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        # define the logic of your computational graph\n",
    "\n",
    "        ################\n",
    "        # Run the convolutional blocks\n",
    "        x = self.conv1(input_data)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear(x)\n",
    "        predictions = self.softmax(logits)\n",
    "        return predictions\n",
    "\n",
    "        # data = data.view(data.shape[0], -1)\n",
    "        # data = self.lin(data)\n",
    "        # data = self.softmax(data)\n",
    "\n",
    "        #return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 66, 46]             160\n",
      "              ReLU-2           [-1, 16, 66, 46]               0\n",
      "         MaxPool2d-3           [-1, 16, 33, 23]               0\n",
      "       BatchNorm2d-4           [-1, 16, 33, 23]              32\n",
      "            Conv2d-5           [-1, 32, 35, 25]           4,640\n",
      "              ReLU-6           [-1, 32, 35, 25]               0\n",
      "         MaxPool2d-7           [-1, 32, 17, 12]               0\n",
      "       BatchNorm2d-8           [-1, 32, 17, 12]              64\n",
      "            Conv2d-9           [-1, 64, 19, 14]          18,496\n",
      "             ReLU-10           [-1, 64, 19, 14]               0\n",
      "        MaxPool2d-11             [-1, 64, 9, 7]               0\n",
      "      BatchNorm2d-12             [-1, 64, 9, 7]             128\n",
      "           Conv2d-13           [-1, 128, 11, 9]          73,856\n",
      "             ReLU-14           [-1, 128, 11, 9]               0\n",
      "        MaxPool2d-15            [-1, 128, 5, 4]               0\n",
      "      BatchNorm2d-16            [-1, 128, 5, 4]             256\n",
      "          Flatten-17                 [-1, 2560]               0\n",
      "           Linear-18                   [-1, 10]          25,610\n",
      "          Softmax-19                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 123,242\n",
      "Trainable params: 123,242\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 2.03\n",
      "Params size (MB): 0.47\n",
      "Estimated Total Size (MB): 2.51\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "cnn = CLModel()\n",
    "from torchsummary import summary\n",
    "summary(cnn, (1, 64, 44))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(data, batch_size:int, shuffle:bool=False) -> torch.utils.data.DataLoader:\n",
    "    ''' creating dataloader  \n",
    "            Arguments:\n",
    "                batch_size: int\n",
    "                    number of samples to process for the model\n",
    "                shuffle: bool\n",
    "                    whether to shuffle dataset. \n",
    "    '''\n",
    "    dataloader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=shuffle)\n",
    "    return dataloader\n",
    "\n",
    "def train_single_epoch(\n",
    "    model:nn.Module, \n",
    "    data_loader:torch.utils.data.DataLoader, \n",
    "    loss_fn:torch.nn.modules.loss, \n",
    "    optimiser:torch.optim,\n",
    "    metrics,\n",
    "    device:torch.device\n",
    "):\n",
    "    ''' method to perform single epoch of training '''\n",
    "\n",
    "    rolling_loss = 0.\n",
    "    #rolling_metric = 0.\n",
    "    rolling_metric = metrics()\n",
    "    for input, target in data_loader:\n",
    "        input, target = input.to(device), target.to(device)\n",
    "        ################\n",
    "\n",
    "        # YOUR CODE HERE \n",
    "        optimiser.zero_grad()\n",
    "        # step 1: pass input tensor through the model\n",
    "        output = cnn(input)\n",
    "\n",
    "        # step 2: compute loss\n",
    "        loss = loss_fn(output, target)\n",
    "\n",
    "        # step 3: update optimizer and do a backward for the loss function\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        #print(f\"loss: {loss.item()}\")\n",
    "\n",
    "        _, prediction = torch.max(output,1)\n",
    "        # step 5 (final): compute train epoch loss and metrics\n",
    "        rolling_loss += loss.item()\n",
    "        rolling_metric(prediction, target)\n",
    "        #rolling_metric += (prediction == target ).float().sum()\n",
    "        #metric = metrics(prediction, target)\n",
    "        #print(f\"metric: {metric}\")\n",
    "        #rolling_metric += metric\n",
    "\n",
    "    ################\n",
    "    total_rolling_metric = rolling_metric.compute()\n",
    "    print(f'Train loss: {rolling_loss} | Train metric: {total_rolling_metric}')\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    model:nn.Module,\n",
    "    val_dataloader:torch.utils.data.DataLoader, \n",
    "    loss_fn:torch.nn.modules.loss, \n",
    "    metrics, # define you type here  \n",
    "    device:torch.device):\n",
    "    ''' method to perform evaluation step '''\n",
    "\n",
    "    rolling_loss = 0.\n",
    "    rolling_metric = metrics()\n",
    "    for input, target in val_dataloader: \n",
    "        input, target = input.to(device), target.to(device)\n",
    "        ################\n",
    "\n",
    "        # YOUR CODE HERE \n",
    "\n",
    "        # step 1: pass input tensor through the model\n",
    "        output = cnn(input)\n",
    "        # step 2: compute loss and metrics\n",
    "        loss = loss_fn(output, target)\n",
    "        _, prediction = torch.max(output,1)\n",
    "\n",
    "        rolling_loss += loss.item()\n",
    "        #rolling_metric += metrics(prediction, target)\n",
    "        rolling_metric(prediction, target)\n",
    "    ################\n",
    "    total_rolling_metric = rolling_metric.compute()\n",
    "\n",
    "    print(f'Validation loss: {rolling_loss} | Validation metric: {total_rolling_metric}')\n",
    "    return rolling_loss, total_rolling_metric\n",
    "    \n",
    "\n",
    "def train(model, train_dataloader, val_dataloader, loss_fn, metrics, optimiser, device, epochs):\n",
    "    for i in range(epochs):\n",
    "        print(f\"Epoch {i+1}\")\n",
    "        train_single_epoch(model, train_dataloader, loss_fn, optimiser, metrics, device)\n",
    "        evaluate(model, val_dataloader, loss_fn, metrics, device)\n",
    "    print(\"Finished training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Train loss: 894.4097723960876 | Train metric: 0.4136005640029907\n",
      "Validation loss: 112.88241708278656 | Validation metric: 0.4192439913749695\n",
      "Epoch 2\n",
      "Train loss: 885.6427943706512 | Train metric: 0.4346456825733185\n",
      "Validation loss: 112.15002286434174 | Validation metric: 0.43184420466423035\n",
      "Epoch 3\n",
      "Train loss: 879.6404361724854 | Train metric: 0.45139583945274353\n",
      "Validation loss: 111.51596426963806 | Validation metric: 0.44558992981910706\n",
      "Epoch 4\n",
      "Train loss: 875.0382670164108 | Train metric: 0.46499642729759216\n",
      "Validation loss: 110.97851824760437 | Validation metric: 0.4501718282699585\n",
      "Epoch 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_22232/2903818520.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[1;31m# train model\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 13\u001B[1;33m \u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcnn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_dataloader\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mval_dataloader\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mloss_fn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmetrics\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moptimiser\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mEPOCHS\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     14\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[1;31m# save model\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_22232/3261984987.py\u001B[0m in \u001B[0;36mtrain\u001B[1;34m(model, train_dataloader, val_dataloader, loss_fn, metrics, optimiser, device, epochs)\u001B[0m\n\u001B[0;32m     90\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mepochs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     91\u001B[0m         \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"Epoch {i+1}\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 92\u001B[1;33m         \u001B[0mtrain_single_epoch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_dataloader\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mloss_fn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moptimiser\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmetrics\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     93\u001B[0m         \u001B[0mevaluate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mval_dataloader\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mloss_fn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmetrics\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     94\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Finished training\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_22232/3261984987.py\u001B[0m in \u001B[0;36mtrain_single_epoch\u001B[1;34m(model, data_loader, loss_fn, optimiser, metrics, device)\u001B[0m\n\u001B[0;32m     23\u001B[0m     \u001B[1;31m#rolling_metric = 0.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     24\u001B[0m     \u001B[0mrolling_metric\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmetrics\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 25\u001B[1;33m     \u001B[1;32mfor\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtarget\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mdata_loader\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     26\u001B[0m         \u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtarget\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtarget\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     27\u001B[0m         \u001B[1;31m################\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\olexandr\\.virtualenvs\\dataspellenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001B[0m in \u001B[0;36m__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    626\u001B[0m                 \u001B[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    627\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_reset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# type: ignore[call-arg]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 628\u001B[1;33m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_next_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    629\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_num_yielded\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    630\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_dataset_kind\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0m_DatasetKind\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mIterable\u001B[0m \u001B[1;32mand\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\olexandr\\.virtualenvs\\dataspellenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001B[0m in \u001B[0;36m_next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    669\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_next_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    670\u001B[0m         \u001B[0mindex\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_next_index\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# may raise StopIteration\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 671\u001B[1;33m         \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_dataset_fetcher\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfetch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mindex\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# may raise StopIteration\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    672\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_pin_memory\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    673\u001B[0m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_utils\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_pin_memory_device\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\olexandr\\.virtualenvs\\dataspellenv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001B[0m in \u001B[0;36mfetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     56\u001B[0m                 \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__getitems__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     57\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 58\u001B[1;33m                 \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0midx\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0midx\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     59\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     60\u001B[0m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\olexandr\\.virtualenvs\\dataspellenv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     56\u001B[0m                 \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__getitems__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     57\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 58\u001B[1;33m                 \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0midx\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0midx\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     59\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     60\u001B[0m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\olexandr\\.virtualenvs\\dataspellenv\\lib\\site-packages\\torch\\utils\\data\\dataset.py\u001B[0m in \u001B[0;36m__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m    293\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0midx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    294\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mindices\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[1;32min\u001B[0m \u001B[0midx\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 295\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mindices\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0midx\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    296\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    297\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__len__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_22232/1266679709.py\u001B[0m in \u001B[0;36m__getitem__\u001B[1;34m(self, index)\u001B[0m\n\u001B[0;32m     20\u001B[0m         \u001B[0maudio_sample_path\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_get_audio_sample_path\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mindex\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     21\u001B[0m         \u001B[0mlabel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_get_audio_sample_label\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mindex\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 22\u001B[1;33m         \u001B[0msignal\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msr\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorchaudio\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0maudio_sample_path\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     23\u001B[0m         \u001B[0msignal\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msignal\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     24\u001B[0m         \u001B[0msignal\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_resample_if_necessary\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msignal\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\olexandr\\.virtualenvs\\dataspellenv\\lib\\site-packages\\torchaudio\\backend\\soundfile_backend.py\u001B[0m in \u001B[0;36mload\u001B[1;34m(filepath, frame_offset, num_frames, normalize, channels_first, format)\u001B[0m\n\u001B[0;32m    212\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    213\u001B[0m         \u001B[0mframes\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfile_\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_prepare_read\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mframe_offset\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnum_frames\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 214\u001B[1;33m         \u001B[0mwaveform\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfile_\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mframes\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0malways_2d\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    215\u001B[0m         \u001B[0msample_rate\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfile_\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msamplerate\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    216\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\olexandr\\.virtualenvs\\dataspellenv\\lib\\site-packages\\soundfile.py\u001B[0m in \u001B[0;36mread\u001B[1;34m(self, frames, dtype, always_2d, fill_value, out)\u001B[0m\n\u001B[0;32m    890\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mframes\u001B[0m \u001B[1;33m<\u001B[0m \u001B[1;36m0\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mframes\u001B[0m \u001B[1;33m>\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mout\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    891\u001B[0m                 \u001B[0mframes\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mout\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 892\u001B[1;33m         \u001B[0mframes\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_array_io\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'read'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mout\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mframes\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    893\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mout\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m>\u001B[0m \u001B[0mframes\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    894\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mfill_value\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\olexandr\\.virtualenvs\\dataspellenv\\lib\\site-packages\\soundfile.py\u001B[0m in \u001B[0;36m_array_io\u001B[1;34m(self, action, array, frames)\u001B[0m\n\u001B[0;32m   1339\u001B[0m         \u001B[1;32massert\u001B[0m \u001B[0marray\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mitemsize\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0m_ffi\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msizeof\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mctype\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1340\u001B[0m         \u001B[0mcdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_ffi\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcast\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mctype\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;34m'*'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0marray\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__array_interface__\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'data'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1341\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_cdata_io\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0maction\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcdata\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mctype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mframes\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1342\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1343\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_cdata_io\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maction\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mctype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mframes\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\olexandr\\.virtualenvs\\dataspellenv\\lib\\site-packages\\soundfile.py\u001B[0m in \u001B[0;36m_cdata_io\u001B[1;34m(self, action, data, ctype, frames)\u001B[0m\n\u001B[0;32m   1348\u001B[0m             \u001B[0mcurr\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtell\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1349\u001B[0m         \u001B[0mfunc\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_snd\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'sf_'\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0maction\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;34m'f_'\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mctype\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1350\u001B[1;33m         \u001B[0mframes\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_file\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mframes\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1351\u001B[0m         \u001B[0m_error_check\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_errorcode\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1352\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mseekable\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# Define your loss function, optimization algorithm, and a metric function(s)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adam(cnn.parameters(),lr=LEARNING_RATE)\n",
    "metrics = torchmetrics.Accuracy\n",
    "\n",
    "train_dataloader = create_data_loader(train_dataset, BATCH_SIZE)\n",
    "val_dataloader = create_data_loader(val_dataset, BATCH_SIZE)\n",
    "test_dataloader = create_data_loader(test_dataset, BATCH_SIZE)\n",
    "\n",
    "\n",
    "# train model\n",
    "train(cnn, train_dataloader, val_dataloader, loss_fn, metrics, optimiser, device, EPOCHS)\n",
    "\n",
    "# save model\n",
    "torch.save(cnn.state_dict(), \"feedforwardnet.pth\")\n",
    "print(\"Trained feed forward net saved at feedforwardnet.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# evaluate your model on test split\n",
    "\n",
    "test_loss, test_accuracy =  evaluate(cnn, test_dataloader, loss_fn, metrics, device)\n",
    "val_loss, val_accuracy = evaluate(cnn, val_dataloader, loss_fn, metrics, device)\n",
    "\n",
    "print(f'Test loss: {test_loss} | Test accuracy: {test_accuracy}')\n",
    "print(f'Validation loss: {test_loss} | Validation accuracy: {test_accuracy}')\n",
    "################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "1) Write a litter summary of the audio transformation methods (1-2 sentences), and explore them on your own\n",
    "\n",
    "2) Create a table with the model performance comparison. The table should include short model summary (ex. 4xConvBlocks - Flatten - Linear - Softmax), comments about hyperparameters (ex. ConvBlocks: [64, 64, 64, 64] - Dropout: 0.2, etc.) and metrics values for validation and test datasets.\n",
    "\n",
    "3) Train baseline model with 4x ConvBlocks - Flatten - Linear - Softmax\n",
    "\n",
    "4) Train baseline model with BatchNorm\n",
    "\n",
    "5) Train baseline model with BatchNorm and Dropout\n",
    "\n",
    "6) Train 5 model models with different hyperparameters\n",
    "\n",
    "7) Add all models to a table \n",
    "\n",
    "8) Write an explanation to the table and indicate the best model parameters and architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0adcc2737ebf6a4a119f135174df96668767fca1ef1112612db5ecadf2b6d608"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}