{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'sox_io'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "\n",
    "import pandas as pd\n",
    "import torchmetrics as torchmetrics\n",
    "\n",
    "str(torchaudio.get_audio_backend())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset \n",
    "As part of HW2, you will work with the audio dataset. Download the dataset by the following [link](https://urbansounddataset.weebly.com/download-urbansound8k.html) and remember a path to it. You will have to train a sound classification model, explore different audio transformation methods and try different model architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UrbanSoundDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,\n",
    "                 annotations_file,\n",
    "                 audio_dir,\n",
    "                 transformation,\n",
    "                 target_sample_rate,\n",
    "                 num_samples,\n",
    "                 device):\n",
    "        self.annotations = pd.read_csv(annotations_file)\n",
    "        self.audio_dir = audio_dir\n",
    "        self.device = device\n",
    "        self.transformation = transformation.to(self.device)\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        audio_sample_path = self._get_audio_sample_path(index)\n",
    "        label = self._get_audio_sample_label(index)\n",
    "        signal, sr = torchaudio.load(audio_sample_path)\n",
    "        signal = signal.to(self.device)\n",
    "        signal = self._resample_if_necessary(signal, sr)\n",
    "        signal = self._mix_down_if_necessary(signal)\n",
    "        signal = self._cut_if_necessary(signal)\n",
    "        signal = self._right_pad_if_necessary(signal)\n",
    "        signal = self.transformation(signal)\n",
    "        return signal, label\n",
    "\n",
    "    def _cut_if_necessary(self, signal):\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            signal = signal[:, :self.num_samples]\n",
    "        return signal\n",
    "\n",
    "    def _right_pad_if_necessary(self, signal):\n",
    "        length_signal = signal.shape[1]\n",
    "        if length_signal < self.num_samples:\n",
    "            num_missing_samples = self.num_samples - length_signal\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
    "        return signal\n",
    "\n",
    "    def _resample_if_necessary(self, signal, sr):\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
    "            signal = resampler(signal)\n",
    "        return signal\n",
    "\n",
    "    def _mix_down_if_necessary(self, signal):\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        return signal\n",
    "\n",
    "    def _get_audio_sample_path(self, index):\n",
    "        fold = f\"fold{self.annotations.iloc[index, 5]}\"\n",
    "        path = os.path.join(self.audio_dir, fold, self.annotations.iloc[\n",
    "            index, 0])\n",
    "        return path\n",
    "\n",
    "    def _get_audio_sample_label(self, index):\n",
    "        return self.annotations.iloc[index, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the dataset \n",
    "ANNOTATIONS_FILE = \"datasets/UrbanSound8K/metadata/UrbanSound8K.csv\"\n",
    "AUDIO_DIR = \"datasets/UrbanSound8K/audio\"\n",
    "\n",
    "# audio signal sample rate \n",
    "SAMPLE_RATE = 22050\n",
    "# max number of samples in audio\n",
    "NUM_SAMPLES = 22050\n",
    "\n",
    "# optimizer learning rate\n",
    "LEARNING_RATE = 1e-5\n",
    "# number of train epochs\n",
    "EPOCHS = 10\n",
    "# number of samples in each batch size\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining audio preprocessing function. Refer to torchaudio.transforms to \n",
    "# check Spectrogram(), MFCC() and LFCC() transfrormations. Retrain the model\n",
    "# with each transformation and write your metrics resulst and conclusaions.\n",
    "mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=SAMPLE_RATE,\n",
    "        n_fft=1024,\n",
    "        hop_length=512,\n",
    "        n_mels=64\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset len:  8732\n",
      "Train split: 6985 | Validation split: 873 | Test split: 874\n"
     ]
    }
   ],
   "source": [
    "# defining dataset class with \n",
    "dataset = UrbanSoundDataset(\n",
    "    ANNOTATIONS_FILE,\n",
    "    AUDIO_DIR,\n",
    "    mel_spectrogram,\n",
    "    SAMPLE_RATE,\n",
    "    NUM_SAMPLES,\n",
    "    device\n",
    ")\n",
    "\n",
    "# calculating validation split sizes\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "val_test_size = len(dataset) - train_size\n",
    "\n",
    "val_size = val_test_size // 2\n",
    "test_size = val_test_size - val_size\n",
    "\n",
    "print('Dataset len: ', len(dataset))\n",
    "print(f'Train split: {train_size} | Validation split: {val_size} | Test split: {test_size}')\n",
    "\n",
    "# splitting original dataset into train and val_test sets\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, val_size + test_size]\n",
    ")\n",
    "# splitting val_test set into val dataset and test dataset\n",
    "val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    val_dataset, [val_size, test_size]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal, label = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Define your model architecture here. \n",
    "\n",
    "1) Build a baseline model that consists of consecutive convolution blocks. Flatten the convolution output and pass through the linear layer with softmax activation function, to obtain class distributions\n",
    "\n",
    "2) Add the BatchNorm layer after each convolution block and compare the results with the baseline model\n",
    "\n",
    "3) Add a Dropout layer after each BatchNorm block and compare the results\n",
    "\n",
    "4) Try different parameters for the blocks:\n",
    " - Conv layer: out_channels, kernel_size, stride\n",
    " - Dropout: p\n",
    " \n",
    " and compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class CLModel to define your model architecture and computational graph\n",
    "class CLModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ################\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        # define you convolution based classification model\n",
    "        # 4 conv blocks / flatten / linear / softmax\n",
    "\n",
    "        ################\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=16,\n",
    "                kernel_size=(5,5),\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            # nn.Dropout(0.1)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=16,\n",
    "                out_channels=32,\n",
    "                kernel_size=(5,5),\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            # nn.Dropout(0.1)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64,\n",
    "                kernel_size=(3,3),\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            # nn.Dropout(0.1)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=128,\n",
    "                kernel_size=(3,3),\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            # nn.Dropout(0.1)\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(2560, 640)\n",
    "        self.linear2 = nn.Linear(640, 10)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "\n",
    "    def forward(self, input_data: torch.Tensor) -> torch.Tensor:\n",
    "        ################\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        # define the logic of your computational graph\n",
    "\n",
    "        ################\n",
    "        # Run the convolutional blocks\n",
    "        x = self.conv1(input_data)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.linear2(x)\n",
    "        predictions = self.softmax(x)\n",
    "        return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Sequential: 1-1                        [-1, 16, 32, 22]          --\n",
      "|    └─Conv2d: 2-1                       [-1, 16, 64, 44]          416\n",
      "|    └─ReLU: 2-2                         [-1, 16, 64, 44]          --\n",
      "|    └─MaxPool2d: 2-3                    [-1, 16, 32, 22]          --\n",
      "|    └─BatchNorm2d: 2-4                  [-1, 16, 32, 22]          32\n",
      "├─Sequential: 1-2                        [-1, 32, 16, 11]          --\n",
      "|    └─Conv2d: 2-5                       [-1, 32, 32, 22]          12,832\n",
      "|    └─ReLU: 2-6                         [-1, 32, 32, 22]          --\n",
      "|    └─MaxPool2d: 2-7                    [-1, 32, 16, 11]          --\n",
      "|    └─BatchNorm2d: 2-8                  [-1, 32, 16, 11]          64\n",
      "├─Sequential: 1-3                        [-1, 64, 9, 6]            --\n",
      "|    └─Conv2d: 2-9                       [-1, 64, 18, 13]          18,496\n",
      "|    └─ReLU: 2-10                        [-1, 64, 18, 13]          --\n",
      "|    └─MaxPool2d: 2-11                   [-1, 64, 9, 6]            --\n",
      "|    └─BatchNorm2d: 2-12                 [-1, 64, 9, 6]            128\n",
      "├─Sequential: 1-4                        [-1, 128, 5, 4]           --\n",
      "|    └─Conv2d: 2-13                      [-1, 128, 11, 8]          73,856\n",
      "|    └─ReLU: 2-14                        [-1, 128, 11, 8]          --\n",
      "|    └─MaxPool2d: 2-15                   [-1, 128, 5, 4]           --\n",
      "|    └─BatchNorm2d: 2-16                 [-1, 128, 5, 4]           256\n",
      "├─Flatten: 1-5                           [-1, 2560]                --\n",
      "├─Linear: 1-6                            [-1, 640]                 1,639,040\n",
      "├─Linear: 1-7                            [-1, 10]                  6,410\n",
      "├─Softmax: 1-8                           [-1, 10]                  --\n",
      "==========================================================================================\n",
      "Total params: 1,751,530\n",
      "Trainable params: 1,751,530\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 22.69\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.90\n",
      "Params size (MB): 6.68\n",
      "Estimated Total Size (MB): 7.59\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\n├─Sequential: 1-1                        [-1, 16, 32, 22]          --\n|    └─Conv2d: 2-1                       [-1, 16, 64, 44]          416\n|    └─ReLU: 2-2                         [-1, 16, 64, 44]          --\n|    └─MaxPool2d: 2-3                    [-1, 16, 32, 22]          --\n|    └─BatchNorm2d: 2-4                  [-1, 16, 32, 22]          32\n├─Sequential: 1-2                        [-1, 32, 16, 11]          --\n|    └─Conv2d: 2-5                       [-1, 32, 32, 22]          12,832\n|    └─ReLU: 2-6                         [-1, 32, 32, 22]          --\n|    └─MaxPool2d: 2-7                    [-1, 32, 16, 11]          --\n|    └─BatchNorm2d: 2-8                  [-1, 32, 16, 11]          64\n├─Sequential: 1-3                        [-1, 64, 9, 6]            --\n|    └─Conv2d: 2-9                       [-1, 64, 18, 13]          18,496\n|    └─ReLU: 2-10                        [-1, 64, 18, 13]          --\n|    └─MaxPool2d: 2-11                   [-1, 64, 9, 6]            --\n|    └─BatchNorm2d: 2-12                 [-1, 64, 9, 6]            128\n├─Sequential: 1-4                        [-1, 128, 5, 4]           --\n|    └─Conv2d: 2-13                      [-1, 128, 11, 8]          73,856\n|    └─ReLU: 2-14                        [-1, 128, 11, 8]          --\n|    └─MaxPool2d: 2-15                   [-1, 128, 5, 4]           --\n|    └─BatchNorm2d: 2-16                 [-1, 128, 5, 4]           256\n├─Flatten: 1-5                           [-1, 2560]                --\n├─Linear: 1-6                            [-1, 640]                 1,639,040\n├─Linear: 1-7                            [-1, 10]                  6,410\n├─Softmax: 1-8                           [-1, 10]                  --\n==========================================================================================\nTotal params: 1,751,530\nTrainable params: 1,751,530\nNon-trainable params: 0\nTotal mult-adds (M): 22.69\n==========================================================================================\nInput size (MB): 0.01\nForward/backward pass size (MB): 0.90\nParams size (MB): 6.68\nEstimated Total Size (MB): 7.59\n=========================================================================================="
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn = CLModel()\n",
    "from torchsummary import summary\n",
    "summary(cnn, (1, 64, 44))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(data, batch_size:int, shuffle:bool=False) -> torch.utils.data.DataLoader:\n",
    "    ''' creating dataloader  \n",
    "            Arguments:\n",
    "                batch_size: int\n",
    "                    number of samples to process for the model\n",
    "                shuffle: bool\n",
    "                    whether to shuffle dataset. \n",
    "    '''\n",
    "    dataloader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=shuffle)\n",
    "    return dataloader\n",
    "\n",
    "def train_single_epoch(\n",
    "    model:nn.Module, \n",
    "    data_loader:torch.utils.data.DataLoader, \n",
    "    loss_fn:torch.nn.modules.loss, \n",
    "    optimiser:torch.optim,\n",
    "    metrics,\n",
    "    device:torch.device\n",
    "):\n",
    "    ''' method to perform single epoch of training '''\n",
    "\n",
    "    rolling_loss = 0.\n",
    "    #rolling_metric = 0.\n",
    "    rolling_metric = metrics()\n",
    "    for input, target in data_loader:\n",
    "        input, target = input.to(device), target.to(device)\n",
    "        ################\n",
    "\n",
    "        # YOUR CODE HERE \n",
    "        optimiser.zero_grad()\n",
    "        # step 1: pass input tensor through the model\n",
    "        output = cnn(input)\n",
    "\n",
    "        # step 2: compute loss\n",
    "        loss = loss_fn(output, target)\n",
    "\n",
    "        # step 3: update optimizer and do a backward for the loss function\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        #print(f\"loss: {loss.item()}\")\n",
    "\n",
    "        _, prediction = torch.max(output,1)\n",
    "        # step 5 (final): compute train epoch loss and metrics\n",
    "        rolling_loss += loss.item()\n",
    "        rolling_metric(prediction, target)\n",
    "        #rolling_metric += (prediction == target ).float().sum()\n",
    "        #metric = metrics(prediction, target)\n",
    "        #print(f\"metric: {metric}\")\n",
    "        #rolling_metric += metric\n",
    "\n",
    "    ################\n",
    "    total_rolling_metric = rolling_metric.compute()\n",
    "    print(f'Train loss: {rolling_loss} | Train metric: {total_rolling_metric}')\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    model:nn.Module,\n",
    "    val_dataloader:torch.utils.data.DataLoader, \n",
    "    loss_fn:torch.nn.modules.loss, \n",
    "    metrics, # define you type here  \n",
    "    device:torch.device):\n",
    "    ''' method to perform evaluation step '''\n",
    "\n",
    "    rolling_loss = 0.\n",
    "    rolling_metric = metrics()\n",
    "    for input, target in val_dataloader: \n",
    "        input, target = input.to(device), target.to(device)\n",
    "        ################\n",
    "\n",
    "        # YOUR CODE HERE \n",
    "\n",
    "        # step 1: pass input tensor through the model\n",
    "        output = cnn(input)\n",
    "        # step 2: compute loss and metrics\n",
    "        loss = loss_fn(output, target)\n",
    "        _, prediction = torch.max(output,1)\n",
    "\n",
    "        rolling_loss += loss.item()\n",
    "        #rolling_metric += metrics(prediction, target)\n",
    "        rolling_metric(prediction, target)\n",
    "    ################\n",
    "    total_rolling_metric = rolling_metric.compute()\n",
    "\n",
    "    print(f'Validation loss: {rolling_loss} | Validation metric: {total_rolling_metric}')\n",
    "    return rolling_loss, total_rolling_metric\n",
    "    \n",
    "\n",
    "def train(model, train_dataloader, val_dataloader, loss_fn, metrics, optimiser, device, epochs):\n",
    "    for i in range(epochs):\n",
    "        print(f\"Epoch {i+1}\")\n",
    "        train_single_epoch(model, train_dataloader, loss_fn, optimiser, metrics, device)\n",
    "        evaluate(model, val_dataloader, loss_fn, metrics, device)\n",
    "    print(\"Finished training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Train loss: 963.6666544675827 | Train metric: 0.2887616455554962\n",
      "Validation loss: 116.1071652173996 | Validation metric: 0.3814432919025421\n",
      "Epoch 2\n",
      "Train loss: 914.8163659572601 | Train metric: 0.39398711919784546\n",
      "Validation loss: 112.14015352725983 | Validation metric: 0.44902634620666504\n",
      "Epoch 3\n",
      "Train loss: 890.815515756607 | Train metric: 0.44309234619140625\n",
      "Validation loss: 110.24187421798706 | Validation metric: 0.46620848774909973\n",
      "Epoch 4\n",
      "Train loss: 875.8924663066864 | Train metric: 0.47659268975257874\n",
      "Validation loss: 109.08658397197723 | Validation metric: 0.4902634620666504\n",
      "Epoch 5\n",
      "Train loss: 863.1503790616989 | Train metric: 0.5065139532089233\n",
      "Validation loss: 108.01703226566315 | Validation metric: 0.5154638886451721\n",
      "Epoch 6\n",
      "Train loss: 852.282595038414 | Train metric: 0.53214031457901\n",
      "Validation loss: 107.21808457374573 | Validation metric: 0.5234822630882263\n",
      "Epoch 7\n",
      "Train loss: 843.9744075536728 | Train metric: 0.5507516264915466\n",
      "Validation loss: 106.57804119586945 | Validation metric: 0.5395188927650452\n",
      "Epoch 8\n",
      "Train loss: 836.8573435544968 | Train metric: 0.5680744647979736\n",
      "Validation loss: 106.04816114902496 | Validation metric: 0.544100821018219\n",
      "Epoch 9\n",
      "Train loss: 830.4260458946228 | Train metric: 0.5819613337516785\n",
      "Validation loss: 105.53825390338898 | Validation metric: 0.5589919686317444\n",
      "Epoch 10\n",
      "Train loss: 824.7135388851166 | Train metric: 0.5959914326667786\n",
      "Validation loss: 105.08453261852264 | Validation metric: 0.5612829327583313\n",
      "Finished training\n",
      "Trained feed forward net saved at feedforwardnet.pth\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# Define your loss function, optimization algorithm, and a metric function(s)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adam(cnn.parameters(),lr=LEARNING_RATE)\n",
    "metrics = torchmetrics.Accuracy\n",
    "\n",
    "train_dataloader = create_data_loader(train_dataset, BATCH_SIZE)\n",
    "val_dataloader = create_data_loader(val_dataset, BATCH_SIZE)\n",
    "test_dataloader = create_data_loader(test_dataset, BATCH_SIZE)\n",
    "\n",
    "\n",
    "# train model\n",
    "train(cnn, train_dataloader, val_dataloader, loss_fn, metrics, optimiser, device, EPOCHS)\n",
    "\n",
    "# save model\n",
    "torch.save(cnn.state_dict(), \"feedforwardnet.pth\")\n",
    "print(\"Trained feed forward net saved at feedforwardnet.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 106.35685575008392 | Validation metric: 0.5469107627868652\n",
      "Validation loss: 105.08453261852264 | Validation metric: 0.5612829327583313\n",
      "Test loss: 106.35685575008392 | Test accuracy: 0.5469107627868652\n",
      "Validation loss: 106.35685575008392 | Validation accuracy: 0.5612829327583313\n"
     ]
    }
   ],
   "source": [
    "################\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# evaluate your model on test split\n",
    "\n",
    "test_loss, test_accuracy =  evaluate(cnn, test_dataloader, loss_fn, metrics, device)\n",
    "val_loss, val_accuracy = evaluate(cnn, val_dataloader, loss_fn, metrics, device)\n",
    "\n",
    "print(f'Test loss: {test_loss} | Test accuracy: {test_accuracy}')\n",
    "print(f'Validation loss: {test_loss} | Validation accuracy: {val_accuracy}')\n",
    "################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "1) Write a litter summary of the audio transformation methods (1-2 sentences), and explore them on your own\n",
    "\n",
    "2) Create a table with the model performance comparison. The table should include short model summary (ex. 4xConvBlocks - Flatten - Linear - Softmax), comments about hyperparameters (ex. ConvBlocks: [64, 64, 64, 64] - Dropout: 0.2, etc.) and metrics values for validation and test datasets.\n",
    "\n",
    "3) Train baseline model with 4x ConvBlocks - Flatten - Linear - Softmax\n",
    "\n",
    "4) Train baseline model with BatchNorm\n",
    "\n",
    "5) Train baseline model with BatchNorm and Dropout\n",
    "\n",
    "6) Train 5 model models with different hyperparameters\n",
    "\n",
    "7) Add all models to a table \n",
    "\n",
    "8) Write an explanation to the table and indicate the best model parameters and architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "![alt text](img.png \"Model stats\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " The model with big amount of params wasnt effective, as it needed a lot of training, and the precision difference was not much different from other models. On the other hand, model with least amount of params was not able to train even after 16 epochs. Models with big percentage of dropout tend to train worse than other models. The best architecture is 4 conv layers with MaxPool and BatchNorm and then two consecutive linear layers."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0adcc2737ebf6a4a119f135174df96668767fca1ef1112612db5ecadf2b6d608"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}